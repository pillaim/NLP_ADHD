{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import sklearn\n",
    "import os\n",
    "import shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for importing required functions for data processing\n",
    "\n",
    "sys.path.append(\"data_processing\")\n",
    "sys.path.append(\"data_processing\")\n",
    "\n",
    "# Functions required for data processing\n",
    "\n",
    "import process_text\n",
    "import transform_textfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859fd35",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd6eaf",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05111cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractOriginalText2(input_path):\n",
    "    # read in progress note from txt file (same txt file that was imported to CLAMP for annotation)\n",
    "    results = glob.glob(f\"{input_path}/*.txt\")\n",
    "    rows = []\n",
    "    \n",
    "    # extract original note text from the progress note and save as dataframe\n",
    "    for i in range(len(results)):\n",
    "        row = pd.read_csv(results[i], sep=\"\\t\", quoting=csv.QUOTE_NONE).iloc[:, [6]]\n",
    "        row.columns = ['note_des']\n",
    "        row['file'] = os.path.basename(results[i]).split(\".\")[0]\n",
    "        rows.append(row)\n",
    "    return pd.concat(rows)\n",
    "\n",
    "def extractXMIAnnotation2(input_path):\n",
    "    # write Patterns\n",
    "    id_match = re.compile(\"(?<=xmi:id=\\\")\\d{,5}(?=\\\")\")\n",
    "    begin_match = re.compile(\"(?<=begin=\\\")\\d{,6}(?=\\\")\")\n",
    "    end_match = re.compile(\"(?<=end=\\\")\\d{,6}(?=\\\")\")\n",
    "    tag_match = re.compile(\"(?<=semanticTag=\\\")\\w*(?=\\\")\")\n",
    "\n",
    "    # read annotation file (notes were annotated in CLAMP)\n",
    "    # annotation files were exported from CLAMP in XMI format\n",
    "    results = glob.glob(f\"{input_path}/*.xmi\")\n",
    "    rows = []\n",
    "    \n",
    "    # extract annotations from XMI files (noted with semanticTag field)\n",
    "    for i in range(len(results)):\n",
    "        file = open(results[i], \"r+\")\n",
    "        lines = file.readlines()\n",
    "        lines = lines[0].split('><')\n",
    "        extract = [x for x in lines if re.search(\"semanticTag\", x)]\n",
    "        extract = [(id_match.findall(x),\n",
    "                    begin_match.findall(x),\n",
    "                    end_match.findall(x),\n",
    "                    tag_match.findall(x))\n",
    "                   for x in extract]\n",
    "        unique_tags = sorted(list(set([x[3][0] for x in extract])))\n",
    "#         file_name = re.findall(\"\\\\A.*(?=\\.)\",os.path.basename(results[i]))[0].split(\"-\")\n",
    "        file_name = os.path.basename(results[i]).split(\".\")[0]\n",
    "        \n",
    "        row = pd.DataFrame({'xmi': [os.path.basename(results[i])],\n",
    "                            'file': [os.path.basename(results[i]).split(\".\")[0]],\n",
    "                            'anon_id': [file_name.split(\"-\")[1]],\n",
    "                            'encounter_id': [file_name.split(\"-\")[2]]})\n",
    "\n",
    "        for tag in unique_tags:\n",
    "            row.loc[:, tag] = 1\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    full = pd.concat(rows).fillna(0)\n",
    "\n",
    "    # categorize and process types of ground truth annotations\n",
    "    ## PTBM: parent training in behavioral management\n",
    "    ## weak_bt: weak evidence of PTBM\n",
    "    ## strong_bt: strong evidence of PTBM\n",
    "    ## bt_yn: binary variable for any evidence of PTBM\n",
    "    full['weak_bt'] = np.where((full['Counsel_Handout_BT'] == 1) |\n",
    "                               (full['Counsel_Parent_BT'] == 1), 1, 0)\n",
    "\n",
    "    full['strong_bt'] = np.where((full['Refer_Parent_BT'] == 1) |\n",
    "                                 (full['Refer_School_BT'] == 1), 1, 0)\n",
    "\n",
    "#     full['bt_yn'] = np.where((full['weak_bt'] == 1) |\n",
    "#                              (full['strong_bt'] == 1), 1, 0)\n",
    "\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get required structured data for model in dict\n",
    "## Structure\n",
    "## tabular_data = pd.DataFrame({\n",
    "##     'dis_symp': [0, 1, 1], # 0 = disorder-level code; 1 = symptom-level code\n",
    "##     'age_35_6': [1, 0, 1], # 0 = 3-5 years old; 1 = 6 years old\n",
    "## })\n",
    "# the required variables are hard-coded into the function - make sure to change if needed\n",
    "# patient ID is set as ANON_ID\n",
    "\n",
    "def get_tabular_data(sdata, X_set):\n",
    "    tab = pd.merge(sdata, X_set, left_on='ANON_ID', right_on=X_set.index, how='right')\n",
    "    tab['dis_symp'] = tab['only.symp']\n",
    "    tab['age_35_6'] = tab['ADHD.age'].apply(lambda x: 0 if x < 6 else 1)\n",
    "    tab = tab.set_index('ANON_ID')\n",
    "    tab = tab[['dis_symp', 'age_35_6']].to_dict('records')\n",
    "    \n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataset with note text, structured data, and note labels\n",
    "def get_dataset(text_data, tabular_data, labels):\n",
    "    dataset = [\n",
    "        {\n",
    "            'input_ids': text_data['input_ids'][idx],\n",
    "            'attention_mask': text_data['attention_mask'][idx],\n",
    "            'tabular_data': torch.tensor(list(sample.values()), dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        for idx, sample in enumerate(tabular_data)\n",
    "    ]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bf6bc",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metrics for comparison\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=1)\n",
    "    probabilities = torch.sigmoid(torch.tensor(p.predictions)).cpu().numpy()\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, probabilities[:, 1])\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc49148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get precision, recall, and f1-score at each threshold --> output as dataframe\n",
    "def precision_recall_metrics(true_label , pred_prob):\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(true_label, pred_prob)\n",
    "    precision = precision[:-1]\n",
    "    recall = recall[:-1]\n",
    "    f1 = 2*(precision*recall)/(precision+recall)\n",
    "    results_DF = pd.DataFrame(data = {'precision': precision, \n",
    "                                       'recall': recall,\n",
    "                                       'f1' : f1,\n",
    "                                      'thresholds':thresholds})\n",
    "    print(results_DF)\n",
    "    return results_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce confusion matrix after threshold selection\n",
    "def confusion_matrix_thr(threshold_final, true_label, pred_prob):\n",
    "    \n",
    "    pred_label = [0 if x < threshold_final else 1 for x in pred_prob]\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(true_label, pred_label, normalize='true').ravel()\n",
    "    print(sklearn.metrics.classification_report(true_label, pred_label))\n",
    "\n",
    "    print(\"tn:\",tn)\n",
    "    print(\"tp:\",tp)\n",
    "    print(\"fn:\",fn)\n",
    "    print(\"fp:\",fp)\n",
    "    \n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "def get_auc_ci(y_pred, y_true):\n",
    "    alpha = .95\n",
    "\n",
    "    print(y_true.sum())\n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        y_true,\n",
    "        y_pred)\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc,\n",
    "        scale=auc_std)\n",
    "\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    print('AUC:', auc)\n",
    "    print('AUC COV:', auc_cov)\n",
    "    print('95% AUC CI:', ci)\n",
    "    \n",
    "    return auc, auc_cov, ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-rocket",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_interest = \"BT_yn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull and process original text data (takes in a directory of progress note txt files)\n",
    "originalTextData = extractOriginalText2(\"cohort_2to6/Text files/combined_text\")\n",
    "print(originalTextData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull and process annotations (takes in a directory of CLAMP outputted annotation XMI files)\n",
    "annotatedXMIs = extractXMIAnnotation2(\"cohort_2to6/XMI files/combined\")\n",
    "print(annotatedXMIs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set binary outcome variable\n",
    "\n",
    "annotatedXMIs['BT_yn'] = np.where((annotatedXMIs['Counsel_Parent_BT'] == 1) | (annotatedXMIs['Counsel_Handout_BT'] == 1) | (annotatedXMIs['Refer_Parent_BT'] == 1) | (annotatedXMIs['Refer_School_BT'] == 1), 1, 0)\n",
    "annotatedXMIs['BT_yn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data from both files \n",
    "data = originalTextData.merge(annotatedXMIs, on = \"file\", how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function sectionize() from process_text for processing notes text data\n",
    "\n",
    "data['extractText'] = data['note_des'].apply(lambda x: process_text.sectionize(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function clean_text() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['extractText'].apply(lambda x: process_text.clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in structured data (not included in repository due to PHI)\n",
    "structured_data = pd.read_csv(\"bt_demographics.csv\")\n",
    "# structured_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set patient ID type to int\n",
    "data.anon_id = data.anon_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge text data with structured data to get complete dataset\n",
    "data = pd.merge(structured_data, data, left_on='ANON_ID', right_on='anon_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter down columns in dataset to those necessary for analysis:\n",
    "## text and label\n",
    "data = data.loc[:, ['extractText',label_of_interest, 'ANON_ID']]\\\n",
    "       .rename(columns = {'extractText':'text',\n",
    "                          label_of_interest: 'label'})\n",
    "\n",
    "data = data.set_index('ANON_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, 'text']\n",
    "y = data.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-timer",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validation, and test sets (stratified)\n",
    "## 70/30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 117, stratify = y)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, test_size = 0.3, random_state = 117, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = pd.concat([X_val_train, y_val_train], axis = 1)\n",
    "val_test = pd.concat([X_val_test, y_val_test], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-acting",
   "metadata": {},
   "source": [
    " #Checking the final size for train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \", X_val_train.shape)\n",
    "print(\"X_val shape: \", X_val_test.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in train set\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in validation set\n",
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in test set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-template",
   "metadata": {},
   "source": [
    "# Get Dataset for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce structured dataframe to only include the selected variables\n",
    "red_sdata = structured_data[['ANON_ID', 'ever.dis', 'only.symp', 'ADHD.age']]\n",
    "\n",
    "# train set tabular data\n",
    "train_tab = get_tabular_data(red_sdata, X_val_train)\n",
    "print(train_tab[0])\n",
    "\n",
    "# validation set tabular data\n",
    "val_tab = get_tabular_data(red_sdata, X_val_test)\n",
    "print(val_tab[0])\n",
    "\n",
    "# test set tabular data\n",
    "test_tab = get_tabular_data(red_sdata, X_test)\n",
    "print(test_tab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and get labels in correct format for train and validation sets\n",
    "train_tok_text = tokenizer(list(X_val_train.values), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "train_labels = list(y_val_train)\n",
    "\n",
    "val_tok_text = tokenizer(list(X_val_test.values), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "val_labels = list(y_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('%d GPU(s) available' % torch.cuda.device_count())\n",
    "    print('GPU name: ', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and validation datasets\n",
    "train_dataset = get_dataset(train_tok_text, train_tab, train_labels)\n",
    "valid_dataset = get_dataset(val_tok_text, val_tab, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-spectacular",
   "metadata": {},
   "source": [
    "# Run loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model from checkpoint\n",
    "bert_model = BertForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', num_labels=2)\n",
    "\n",
    "checkpoint = torch.load('./bioclinicalbert_tabular_model_tuned/pytorch_model.bin')\n",
    "args = torch.load('./bioclinicalbert_tabular_model_tuned/training_args.bin')\n",
    "bert_model.load_state_dict(checkpoint)\n",
    "\n",
    "bert_model = bert_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trainer with model and args\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get performance on validation set (should be same as above)\n",
    "val_pred_output = trainer.predict(valid_dataset)\n",
    "val_eval_metrics = trainer.evaluate()\n",
    "val_eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "### output validation performance to file ###\n",
    "# sourceFile = open('./bioclinicalbert_tabular_model_tuned/val_eval_metrics.txt', 'w')\n",
    "# print(val_eval_metrics, file = sourceFile)\n",
    "# sourceFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation probabilities and labels\n",
    "val_probabilities = val_pred_output.predictions\n",
    "val_probabilities = [x[1] for x in np.array([softmax(element) for element in val_probabilities])]\n",
    "val_true_label = val_pred_output.label_ids\n",
    "\n",
    "# put results in dataframe and save as csv\n",
    "val_results = pd.DataFrame()\n",
    "val_results['val_probabilities'] = val_probabilities\n",
    "val_results['val_true_label'] = val_true_label\n",
    "display(val_results.head(1))\n",
    "# val_results.to_csv('./bioclinicalbert_tabular_model_tuned/val_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get precision, recall, and f1-score at each threshold\n",
    "res_df = precision_recall_metrics(val_true_label, val_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get precision recall curve for validation set\n",
    "\n",
    "p, r, t = precision_recall_curve(val_true_label, val_probabilities)\n",
    "auprc = auc(r, p)\n",
    "print(\"AUPRC: \", auprc)\n",
    "\n",
    "plt.figure(dpi=1200) # dpi=1200\n",
    "plt.plot(r, p, 'k', label='BioClinicalBERT (AUC = %s)' % str(auprc.round(2))[0:4])\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "# plt.title(\"Precision-Recall curve\")\n",
    "plt.legend(loc='lower left')\n",
    "# plt.savefig('bioclinical_bert_pr_curve.jpg', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and get labels in correct format for the test set\n",
    "\n",
    "test_tok_text = tokenizer(list(X_test.values), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "test_labels = list(y_test)\n",
    "\n",
    "test_dataset = get_dataset(test_tok_text, test_tab, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results on the test set without using the threshold\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "test_pred_output = trainer.predict(test_dataset)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save test set results ####\n",
    "# sourceFile = open('./bioclinicalbert_tabular_model_tuned/test_eval_metrics.txt', 'w')\n",
    "# print(test_results, file = sourceFile)\n",
    "# sourceFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold selection was done on the validation set\n",
    "\n",
    "threshold = 0.001152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test set results using the threshold\n",
    "\n",
    "test_probabilities = test_pred_output.predictions\n",
    "test_probabilities = [x[1] for x in np.array([softmax(element) for element in test_probabilities])]\n",
    "test_predictions = [0 if x < threshold else 1 for x in test_probabilities]\n",
    "test_true_label = test_pred_output.label_ids\n",
    "print(test_true_label[0], test_predictions[0])\n",
    "\n",
    "# put results in dataframe and save as csv\n",
    "test_results = pd.DataFrame()\n",
    "test_results['test_probabilities'] = test_probabilities\n",
    "test_results['test_true_label'] = test_true_label\n",
    "display(test_results.head(1))\n",
    "# test_results.to_csv('./bioclinicalbert_tabular_model_tuned/test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get confusion matrix for test set after threshold selection\n",
    "test_arr = confusion_matrix_thr(threshold,test_true_label,test_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get AUROC with confidence intervals for model performance on test set\n",
    "auc1, auc_cov1, ci1 = get_auc_ci(pd.Series(test_predictions), pd.Series(test_true_label))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
